
# 如何解析html并在其中找到想要操作的元素

[js渲染](https://juejin.cn/post/6844903815758479374)




## xpath 

用来解析xml文件, 参照语法 https://www.runoob.com/xpath/xpath-syntax.html


https://www.w3school.com.cn/xpath/index.asp

## XPath中text方法和string方法的用法总结
https://blog.csdn.net/qq_27283619/article/details/89154163

https://www.educba.com/xpath-text/



## 如何写xpath ？

打开控制台，选中具体的元素，右键找到copy----->Copy Xpath

自此，爬虫的lxml模块基本了解结束了，应该是对爬虫中的文件操作有了基本的认识了，文笔不好，请多包涵。有任何问题大家可以给我留言。共同探讨和进步。

https://blog.csdn.net/qq_35208583/article/details/89041912

## html文本转化为element对象

将element对象转化为字符串

HtmlStr=etree.tostring(HtmlElement,encoding="utf-8").decode()
print(HtmlStr)

element对象
element对象是xpath语法的使用对象，element对象可由html字符串转化

利用etree.HTML()将html字符串转化为element对象 ,

from lxml import etree
MyStr = '''<meta name="baidu-site-verification" content="cZdR4xxR7RxmM4zE" />
    <meta http-equiv="Pragma" content="no-cache">
    <meta http-equiv="Expires" content="Sun, 6 Mar 2005 01:00:00 GMT">
    '''
HtmlElement = etree.HTML(MyStr) 
print(type(HtmlElement))
#<class 'lxml.etree._Element'>
————————————————
版权声明：本文为CSDN博主「KetchupZ」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/Dch19990825/article/details/87730930



## requests.get()之后得到的源代码跟浏览器里面的不一样

最近在入门爬虫，遇到了一个问题，就是我用requests.get()之后得到的源代码跟浏览器里面的不一样（爬的是百度搜索“美女”之后的网页），今天看了崔庆才的《python3网络爬虫开发实战》在2.3.3节找到了答案：我们用urllib或requests获取到的是HTML源代码，但是这个网页是用js渲染的，上面两个库不会像浏览器那样继续运行后面请求来的js模块，所以不一样。
  还有一个问题，就是我用Beautiful Soup的find_all()方法找不到的标签，用正则表达式找到了，该问题应该是像我这种新手常犯的错误，我是按照F12界面中"元素"的DOM树来写的，这个跟网页源代码不一定一样，因为该界面所展示的东西可能经过JS渲染过，如果要看源代码，就找到F12界面的“源代码”标签，或者“Ctrl+U”查看，再或者在浏览器地址栏https前面输入“view-source”也可以查看。
————————————————
版权声明：本文为CSDN博主「DGXstars」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/weixin_40822925/article/details/106973803

1. 加headers , 把自己的 headers 加在get中就可以爬到和源文件一样的内容了
 https://blog.csdn.net/caodongfang126/article/details/104705869?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ETopBlog-1.topblog&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ETopBlog-1.topblog&utm_relevant_index=1

3. 

 首先要考虑有一些网站是经过请求，然后js处理再渲染出的页面
再者，要考虑你的requests请求头部。是否少了。(这得你自己去尝试)

 


对于一些需要js渲染之后处理后才出现的网页，可以考虑抓取它的数据接口，或者使用PhantomJS Selenium 这类的。将它伪装成一个真正的"浏览器行为"


# selenium 

1. 下载google chrome driver https://sites.google.com/chromium.org/driver/
2. pip install selenimum 

## example 

[爬虫百度example](https://www.cnblogs.com/hkgov/p/13893061.html)



## issue : 找不到element 

出现"找不到element"的问题，添加智能等待，之后问题解决了。

[问题：Selenium WebDriver找不到元素的三种情况](https://blog.csdn.net/zbj18314469395/article/details/83415427)

## selenium 框架或者源码

https://blog.csdn.net/u011035397/article/details/110354710

