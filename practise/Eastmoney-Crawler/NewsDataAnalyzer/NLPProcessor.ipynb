{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然语言识别处理单元\n",
    "## [拂晓工作室](https://github.com/Errrneist/Alchemist)\n",
    "* 此程序可用于识别文字中的语言以及深度分析新闻概况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考资料\n",
    "* [1] [Apple - SFrame实例](https://apple.github.io/turicreate/docs/api/generated/turicreate.SFrame.html#turicreate.SFrame)\n",
    "* [2] [Apple - Turicreate中关于SFrame.apply()的文档](https://apple.github.io/turicreate/docs/api/generated/turicreate.SFrame.apply.html)\n",
    "* [3] [Apple - Turicreate使用TrimRareWords来快速定义高频词](https://apple.github.io/turicreate/docs/api/generated/turicreate.text_analytics.trim_rare_words.html?highlight=remove%20punctuation)\n",
    "* [4] [Apple - 用Turicreate生成Word Count Vector](https://apple.github.io/turicreate/docs/api/generated/turicreate.text_analytics.count_words.html)\n",
    "* [5] [Apple - 关于WordProcessing以及使用dict_trim_by_values(n)删除频率以下的词以及TF-IDF的描述](https://turi.com/learn/userguide/text/analysis.html)\n",
    "* [6] [Stackoverflow - Python从字符串中删除子字符串](https://stackoverflow.com/questions/31273642/better-way-to-remove-multiple-words-from-a-string)\n",
    "* [7] [Stackoverflow - 关于SFrame.apply()和Lambda x的实例](https://stackoverflow.com/questions/33028423/how-can-i-use-apply-with-a-function-that-takes-multiple-inputs)\n",
    "* [8] [Stackexchange - Merge two list and discarding duplicates](https://codereview.stackexchange.com/questions/108171/merge-two-list-and-discarding-duplicates)\n",
    "* [9] [楼宇 - 用Python分析《红楼梦》](https://zhuanlan.zhihu.com/p/29209681)\n",
    "* [10] [zhon - hanzi文档](http://zhon.readthedocs.io/en/latest/)\n",
    "* [11] [CSDN - 使用pyltp包进行中文分词实例](https://blog.csdn.net/sinat_26917383/article/details/77067515)\n",
    "* [12] [CSDN - NLP+语义分析: 角色标注、篇章分析](https://blog.csdn.net/sinat_26917383/article/details/55683599)\n",
    "* [13] [CSDN - NLP情感分析简介](https://blog.csdn.net/android_ruben/article/details/78174172)\n",
    "* [14] [CSDN - 基于机器学习的NLP情感分析（二）—- 分类问题](https://blog.csdn.net/stary_yan/article/details/75330729)\n",
    "* [15] [CSDN - 关于使用zhon.hanzi移除标点符号的解决方法](https://www.cnblogs.com/arkenstone/p/6092255.html)\n",
    "* [16] [CSDN - Python中正则表达式sub函数用法总结](https://blog.csdn.net/hzliyaya/article/details/52495150)\n",
    "* [17] [CSDN - Python中另一种移除标点符号的办法](https://blog.csdn.net/mach_learn/article/details/41744487)\n",
    "* [18] [CSDN - Python字典从大到小排序](https://blog.csdn.net/yangnianjinxin/article/details/79284318)\n",
    "* [19] [CSDN - 中文依存句法简介](https://blog.csdn.net/sinat_33741547/article/details/79258045)\n",
    "* [20] [PYLTP - pyltp技术文档](http://pyltp.readthedocs.io/zh_CN/latest/api.html#id15)\n",
    "* [21] [PYLTP - pyltp介绍文档一](https://www.ltp-cloud.com/intro/#introduction )\n",
    "* [22] [PYLTP - pyltp介绍文档二](http://ltp.readthedocs.io/zh_CN/latest/appendix.html#id5)\n",
    "* [23] [PYLTP - pyltp深度训练模型](https://pan.baidu.com/share/link?shareid=1988562907&uk=2738088569#list/path=/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [A] 导入库模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入基本库\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "# 网络获取相关包\n",
    "import pymysql\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 机器学习与大数据框架\n",
    "import turicreate as tc\n",
    "import csv\n",
    "from zhon.hanzi import punctuation\n",
    "\n",
    "# 自然语言识别框架\n",
    "import pyltp\n",
    "from pyltp import SentenceSplitter\n",
    "from pyltp import Segmentor\n",
    "from pyltp import Postagger\n",
    "from pyltp import NamedEntityRecognizer\n",
    "from pyltp import Parser\n",
    "from pyltp import SementicRoleLabeller\n",
    "LTP_DATA_DIR = '../../../LTP_data_v3.4.0/'  # ltp模型目录的路径\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`pos.model`\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'srl')  # 语义角色标注模型目录路径，模型目录为`srl`。注意该模型路径是一个目录，而不是一个文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# [B] 载入数据模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于从上次停止工作后恢复数据\n",
    "news_path = '../DataSets/Eastmoney/News_NLP/China/'\n",
    "news_source = 'NLPCHINA20180702-1906'\n",
    "# news_source = 'NLPCHINA20180702-1906.csv'\n",
    "news = tc.SFrame(news_path + news_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/hongjunwu/Documents/GitHub/Alchemist/DataSets/Eastmoney/News/China/CHINA20180702-1906.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/hongjunwu/Documents/GitHub/Alchemist/DataSets/Eastmoney/News/China/CHINA20180702-1906.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.051651 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.051651 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,int,int,int,list,str,int]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/hongjunwu/Documents/GitHub/Alchemist/DataSets/Eastmoney/News/China/CHINA20180702-1906.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/hongjunwu/Documents/GitHub/Alchemist/DataSets/Eastmoney/News/China/CHINA20180702-1906.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 509 lines in 0.045534 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 509 lines in 0.045534 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 载入纯净数据\n",
    "news_path = '../DataSets/Eastmoney/News/China/'\n",
    "news_source = 'CHINA20180702-1906.csv'\n",
    "news = tc.SFrame(news_path + news_source)\n",
    "\n",
    "# 预览\n",
    "# news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [C] 检查数据纯净度模块\n",
    "* 测试用，检查清洗数据的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 随机人工检查数据纯净度\n",
    "\n",
    "def showContents(number_to_show, total_amount):\n",
    "    counter = 0\n",
    "    while counter < number_to_show:\n",
    "        print(news['contents'][int(random.uniform(0, total_amount - 1))])\n",
    "        print('')\n",
    "        counter += 1\n",
    "    \n",
    "# 预览\n",
    "# showContents(3, len(news['contents']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [D] 自然语言识别模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] 分句模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分句函数\n",
    "* 将句子分开并创建名为'content_split_sents'的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分句函数\n",
    "def splitSents(contents):\n",
    "    return SentenceSplitter.split(contents)\n",
    "\n",
    "# 执行分句函数\n",
    "news['content_split_sents'] = news['contents'].apply(splitSents)\n",
    "\n",
    "# 预览\n",
    "# news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [2] 移除标点符号模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<input>:3: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:3: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:3: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-4-708563a9aa48>:3: DeprecationWarning: invalid escape sequence \\s\n",
      "  return re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\", contents)\n",
      "<unknown>:2: DeprecationWarning: invalid escape sequence \\s\n"
     ]
    }
   ],
   "source": [
    "# 定义移除标点符号函数\n",
    "def removePunctuation(contents):\n",
    "    return re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\", contents)\n",
    "\n",
    "# 运行移除函数并创建一个名为'contents_nopunc'的列\n",
    "news['contents_nopunc'] = news['contents'].apply(removePunctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] 分词函数模块\n",
    "* 将词分开 并创建名为'contents_split_words'的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词函数\n",
    "def splitWords(contents):\n",
    "    segmentor = Segmentor()  # 初始化实例\n",
    "    segmentor.load(cws_model_path)  # 加载模型\n",
    "    words = segmentor.segment(contents)  # 分词\n",
    "    segmentor.release()  # 释放模型\n",
    "    return words\n",
    "\n",
    "# 执行分词函数\n",
    "news['content_split_words'] = news['contents_nopunc'].apply(splitWords)\n",
    "\n",
    "# 预览\n",
    "# news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] 词性标注模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagWords(contents_split):\n",
    "    postagger = Postagger() # 初始化实例\n",
    "    postagger.load(pos_model_path)  # 加载模型\n",
    "\n",
    "    words = contents_split  # 分词结果\n",
    "    postags = postagger.postag(words)  # 词性标注\n",
    "\n",
    "    postagger.release()  # 释放模型\n",
    "    return postags\n",
    "\n",
    "# 执行标注函数\n",
    "news['content_tag_words'] = news['content_split_words'].apply(tagWords)\n",
    "\n",
    "# 预览\n",
    "# news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5] 命名实体识别模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讲究实体识别\n",
    "def recognizeWords(words):\n",
    "    # postags = news['content_split_words' == words]['content_tag_words']\n",
    "    recognizer = NamedEntityRecognizer() # 初始化实例\n",
    "    recognizer.load(ner_model_path)  # 加载模型\n",
    "    netags = recognizer.recognize(words, news['content_split_words' == words]['content_tag_words'])  # 命名实体识别\n",
    "    recognizer.release()  # 释放模型\n",
    "    return netags\n",
    "\n",
    "# 执行实体识别\n",
    "news['recognized_words'] = news['content_split_words'].apply(recognizeWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [6] 生成Word Count Vector模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成Word Count Vector\n",
    "news['word_count'] = tc.text_analytics.count_words(news['content_split_words'])\n",
    "\n",
    "# 整理Word Count Vector\n",
    "def sortWordCount(dictionary):\n",
    "    return sorted(dictionary.items(),key = lambda d:d[1],reverse=True)\n",
    "\n",
    "# 执行整理Word Count Vector\n",
    "news['sorted_word_count'] = news['word_count'].apply(sortWordCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [7] 计算TF-IDF模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主要生成TF-IDF模块\n",
    "\n",
    "# 生成空格函数\n",
    "def separateSpace(wordlist):\n",
    "    temp_str = ''\n",
    "    for word in wordlist:\n",
    "        temp_str = temp_str + word + ' '\n",
    "    return temp_str\n",
    "\n",
    "# 生成空格组成的str \n",
    "news['contents_space'] = news['content_split_words'].apply(separateSpace)\n",
    "\n",
    "# 通过由空格组成的str计算TF-IDF\n",
    "news['tf-idf'] = tc.text_analytics.tf_idf(news['contents_space'])\n",
    "\n",
    "# 整理TF-IDF\n",
    "def sortTFIDF(dictionary):\n",
    "    return sorted(dictionary.items(),key = lambda d:d[1],reverse=True)\n",
    "\n",
    "# 执行整理TF-IDF\n",
    "news['sorted_tf-idf'] = news['tf-idf'].apply(sortTFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF关键字排名模块\n",
    "def rankKeyTFIDF(tfidf_list):\n",
    "    origional_list = tfidf_list[0:10]\n",
    "    keyWord_list = []\n",
    "    for item in origional_list:\n",
    "        keyWord_list.append(item[0])\n",
    "    return keyWord_list\n",
    "\n",
    "news['ranked_tf-idf_keywords'] = news['sorted_tf-idf'].apply(rankKeyTFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [8] 依存句法分析模块（实验中）\n",
    "* 似乎还不支持保存为pyltp.ParsedWords格式……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依存句法分析函数\n",
    "def parseWords(words):\n",
    "    postags = news['content_split_words' == words]['content_tag_words']\n",
    "    parser = Parser() # 初始化实例\n",
    "    parser.load(par_model_path)  # 加载模型\n",
    "    arcs = parser.parse(words, postags)  # 句法分析\n",
    "    parser.release()  # 释放模型\n",
    "    return arcs\n",
    "\n",
    "# 执行依存句法分析\n",
    "news['parsed_words'] = news['content_split_words'].apply(parseWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [9] 语义角色分析模块（实验中）\n",
    "* 似乎还不支持分析或保存为pyltp.ParsedWords格式……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 语意角色分析函数\n",
    "def labelWords(words):\n",
    "    postags = news['content_split_words' == words]['content_tag_words']\n",
    "    netags = news['content_split_words' == words]['recognized_words']\n",
    "    arcs = news['content_split_words' == words]['parsed_words']\n",
    "    \n",
    "    labeller = SementicRoleLabeller() # 初始化实例\n",
    "    labeller.load(srl_model_path)  # 加载模型\n",
    "    roles = labeller.label(words, postags, netags, arcs)\n",
    "    labeller.release()  # 释放模型\n",
    "    return roles\n",
    "\n",
    "# 执行语意角色分析\n",
    "news['labeled_words'] = news['content_split_words'].apply(labelWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10] 创建关键字模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Merge two list and discarding duplicates [Stackexchange]\n",
    "def merge_no_duplicates(list1, list2):\n",
    "    myset = set(list1).union(set(list2))\n",
    "    return sorted(list(myset))\n",
    "\n",
    "# 将TFIDF筛选出的和从网站本身下载的超链关键字放进一个关键字SArray\n",
    "def makeKeyWords(related):\n",
    "    ranked_tfidf = news['related' == related]['ranked_tf-idf_keywords']\n",
    "    return ranked_tfidf\n",
    "\n",
    "# 执行创建关键字\n",
    "news['keywords'] = news['related'].apply(makeKeyWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['keywords'][305]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [11] 预览SFrame模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10] 随机检查成果模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printRank(num):\n",
    "    print('目前显示第' + str(num) + '条新闻：')\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(news[num]['contents'])\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(news[num]['ranked_tf-idf_keywords'])\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(news[num]['related'])\n",
    "    \n",
    "printRank(random.randint(0,len(news['title'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [11] 保存数据模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存数据\n",
    "filepath = '../DataSets/Eastmoney/News_NLP/China/'\n",
    "news_source = 'NLPCHINA20180702-1906'\n",
    "news.save(filepath + news_source)\n",
    "# news.save(filepath + 'NLP' + news_source, format='csv')\n",
    "\n",
    "# 打印时间戳\n",
    "print('程序运行时间戳：20' \n",
    "      + str(datetime.datetime.now().strftime(\"%y\")) + '年'\n",
    "      + str(datetime.datetime.now().strftime(\"%m\")) + '月' \n",
    "      + str(datetime.datetime.now().strftime(\"%d\")) + '日' \n",
    "      + str(datetime.datetime.now().strftime(\"%H\")) + '时' \n",
    "      + str(datetime.datetime.now().strftime(\"%M\")) + '分' \n",
    "      + str(datetime.datetime.now().strftime(\"%S\")) + '秒')\n",
    "\n",
    "# 打印数据路径\n",
    "print('\\n成功保存自然语言识别数据文件！数据路径：')\n",
    "print(filepath + news_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
